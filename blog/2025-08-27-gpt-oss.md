---
title: "SGLang for gpt-oss: From Day 0 Support to Enhanced Performance"
author: "SGLang Team"
date: "August 27, 2025"
previewImg: /images/blog/glm_4_5/GLM-4-5-preview.png
---

We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss-120b model. **While we had support from day one, we took the last few weeks to enhance our engine to ensure you get the best possible performance.**

This post highlights our latest achievements: a significant performance uplift for gpt-oss with up to <span style="color: red; font-weight: bold;">1.3x higher throughput</span>, out-of-the-box support for NVIDIA Blackwell & Hopper and AMD MI350 GPUs, and enhanced APIs to power complex agentic applicationsâ€”all while maintaining the model's high accuracy.

All changes are now available in our main branch.

### Easy Get Started

```bash
pip install "sglang>=0.5.2"
python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 4
```

<div style="color: red; font-weight: bold;">
link the document / issue for reproduce the results and environment setup.
</div>

## By the Numbers: Comprehensive Benchmark Results ðŸ“Š

To demonstrate the impact of our optimizations, we conducted comprehensive benchmarks across a range of hardware and configurations.

For latency-sensitive applications, we measured single-batch decode throughput across NVIDIA and AMD GPUs, showcasing excellent performance.

<div style="color: red; font-weight: bold;">
<ul>
<li>B200 is tested on TP=4</li>
<li>H100 is tested on TP=8 (triton would be faster, using default fa3)</li>
<li>MI350 is tested on TP=8 for MXFP4 and TP=4 for BF16</li>
</ul>
</div>

| Hardware / Precision | NVIDIA B200  | NVIDIA H100  | AMD MI350    |
| -------------------- | ------------ | ------------ | ------------ |
| MXFP4                | 416.02 tok/s | 290.85 tok/s | 205.76 tok/s |
| BF16                 | 315.63 tok/s | 270.76 tok/s | 209.86 tok/s |

For high-throughput applications, we measured batch decode throughput across NVIDIA and AMD GPUs, showcasing great performance and improvements over our day 0 results.

<div style="color: red; font-weight: bold;">
Make two tables into charts
</div>

B200 Results

| tp size | Input | Prefill TPS          | Decode TPS         |
| ------- | ----- | -------------------- | ------------------ |
| 4       | 1024  | ? -> 160594.58 tok/s | ? -> 7098.99 tok/s |
| 4       | 8192  | ? -> 163333.56 tok/s | ? -> 6629.66 tok/s |

H100 Results

| tp size | Input | Prefill TPS                       | Decode TPS                     |
| ------- | ----- | --------------------------------- | ------------------------------ |
| 8       | 1024  | 81457.00 tok/s -> 92661.70 tok/s  | 4012.36 tok/s -> 4790.95 tok/s |
| 8       | 8192  | 74938.90 tok/s -> 107716.65 tok/s | 3558.63 tok/s -> 4576.74 tok/s |


## Performance Deep Dive ðŸš€ 

<div style="color: red; font-weight: bold;">
Need more check from @Tom@Ke@Lianmin
</div>

Our performance gains come from several key optimizations at the kernel level:
- **FlashInfer Kernels for Blackwell**: To unlock peak performance for gpt-oss on Blackwell GPUs, we have integrated highly optimized kernels from FlashInfer (leveraging TensorRT-LLM). This accelerates core components, including multi-head attention and Mixture of Experts (MoE) layers, on the new hardware.
- **FlashAttention-3 for Hopper**: We modified the FlashAttention-3 kernels to support attention sinks, providing a significant speedup for inference on Hopper GPUs.
- **Kernel Fusion and Reduction**: We performed several low-level fusions to reduce overhead. This includes fusing the RMS norm with all-reduce, merging the set KV buffer operation into RoPE, and fusing hidden states padding into quantization. We also removed unnecessary kernels and reduced CPU overhead for greater efficiency.

## Accuracy Alignment with Official Report ðŸŽ¯

We validated our optimized gpt-oss implementation against the GPQA benchmark and confirmed that our results align closely with the official model card, ensuring that these speedups do not compromise the model's reasoning capabilities.

| Reasoning Effort | SGLang | Official |
| ---------------- | ------ | -------- |
| Low              | 65.0   | 67.1     |
| Medium           | 71.3   | 73.1     |
| High             | 79.8   | 80.1     |

## Powering Agentic Applications ðŸ¤–

To better enable agentic workflows, SGLang now offers native support for tool calls and separating reasoning from the final response. This is compatible with the OpenAI API and allows for more complex, multi-step interactions.

Hereâ€™s a quick example of how to extract the model's reasoning steps:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:30000/v1",
    api_key="EMPTY"
)
model_name = "openai/gpt-oss-120b"

messages = [
    {
        "role": "user",
        "content": "What is 1+3?",
    }
]

response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    stream=False,
    # Use extra_body to request separate reasoning content
    extra_body={"separate_reasoning": True},
)

print("==== Reasoning ====")
print(response.choices[0].message.reasoning_content)

print("\n==== Final Answer ====")
print(response.choices[0].message.content)
```

## What's Next? ðŸ”®

We are continuously working to push the boundaries of LLM inference. Our future roadmap includes exploring SWA (Sliding Window Attention) optimizations and speculative decoding to further enhance performance.

We invite you to try out the latest version of SGLang and share your feedback. Thank you for being part of our community!