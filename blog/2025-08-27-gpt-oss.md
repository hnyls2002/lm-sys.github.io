---
title: "SGLang for gpt-oss: From Day 0 Support to Enhanced Performance"
author: "SGLang Team"
date: "August 27, 2025"
previewImg: /images/blog/glm_4_5/GLM-4-5-preview.png
---

<span style="color: red; font-weight: bold;">
All the red text should be re-checked carefully.
</span>

We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss-120b model. **While we had support from day one, we took the last few weeks to enhance our engine to ensure you get the best possible performance.**

This post highlights our latest achievements: a significant performance uplift for gpt-oss with up to <span style="color: red; font-weight: bold;">1.3x</span> higher throughput, out-of-the-box support for NVIDIA Blackwell & Hopper and AMD MI350 GPUs, and enhanced APIs to power complex agentic applicationsâ€”all while maintaining the model's high accuracy.

All changes are now available in our main branch.

### Get Started with SGLang

```bash
pip install "sglang>=0.5.2"
python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 4
```

<div style="color: red; font-weight: bold;">
Add all necessary commands into the document.
</div>

For detailed instructions on environment setup and how to reproduce our results, please see our guide [here](https://docs.sglang.ai/basic_usage/gpt_oss.html).

## By the Numbers: Comprehensive Benchmark Results ðŸ“Š

To demonstrate the impact of our optimizations, we conducted comprehensive benchmarks across a range of hardware configurations.

##### Low-Latency Performance (Batch Size = 1)

For latency-sensitive applications, we measured single-batch decode throughput across NVIDIA and AMD GPUs, showcasing excellent performance.

| Hardware / Precision | NVIDIA B200  | NVIDIA H100  | AMD MI350    |
| -------------------- | ------------ | ------------ | ------------ |
| MXFP4                | 416.02 tok/s | 290.85 tok/s | 205.76 tok/s |
| BF16                 | 315.63 tok/s | 270.76 tok/s | 209.86 tok/s |

<span style="color: red; font-size: 12px;">
B200 was tested with TP=4, H100 with TP=8, and MI350 with TP=8 for MXFP4 and TP=4 for BF16.
</span>

##### High-Throughput Performance (Batch Size = 32)

For high-throughput applications, SGLang delivers significant performance gains over our initial Day 0 support. <span style="color: red; font-weight: bold;">On NV what GPUs, we ... (Make all the results into charts)</span>

B200 Results

| tp size | Input | Prefill TPS          | Decode TPS         |
| ------- | ----- | -------------------- | ------------------ |
| 4       | 1024  | ? -> 160594.58 tok/s | ? -> 7098.99 tok/s |
| 4       | 8192  | ? -> 163333.56 tok/s | ? -> 6629.66 tok/s |

H100 Results

| tp size | Input | Prefill TPS                       | Decode TPS                     |
| ------- | ----- | --------------------------------- | ------------------------------ |
| 8       | 1024  | 81457.00 tok/s -> 92661.70 tok/s  | 4012.36 tok/s -> 4790.95 tok/s |
| 8       | 8192  | 74938.90 tok/s -> 107716.65 tok/s | 3558.63 tok/s -> 4576.74 tok/s |


## Performance Deep Dive ðŸš€

<div style="color: red; font-weight: bold;">
Need more check from @Tom@Ke@Lianmin
</div>

Our performance gains come from several key optimizations at the kernel level:

- **FlashInfer Kernels for Blackwell**: To unlock peak performance for gpt-oss on Blackwell GPUs, we integrated highly optimized kernels from FlashInfer (leveraging TensorRT-LLM). This accelerates core components, including multi-head attention and Mixture of Experts (MoE) layers, on the new hardware.
- **FlashAttention-3 for Hopper**: We modified the FlashAttention-3 kernels to support attention sinks, providing a significant speedup for inference on Hopper GPUs.
- **Kernel Fusion and Reduction**: We performed several low-level fusions to reduce overhead. This includes fusing the RMS norm with all-reduce, merging the set KV buffer operation into RoPE, and fusing hidden states padding into quantization. We also removed unnecessary kernels and reduced CPU overhead for greater efficiency.

## Accuracy Alignment with Official Report ðŸŽ¯

We validated our optimized gpt-oss implementation against the GPQA benchmark and confirmed that our results align closely with the official model card, ensuring that these speedups do not compromise the model's reasoning capabilities.

| Reasoning Effort | SGLang | Official |
| ---------------- | ------ | -------- |
| Low              | 65.0   | 67.1     |
| Medium           | 71.3   | 73.1     |
| High             | 79.8   | 80.1     |

## Powering Agentic Applications ðŸ¤–

<div style="color: red; font-weight: bold;">
Need more check from @Chang@Lianmin@Xingyuan
</div>

To better enable agentic workflows, SGLang now offers OpenAI Reponse API support and native chat completion support.

Hereâ€™s a quick example of how to use the OpenAI Response API:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:30000/v1",
    api_key="EMPTY"
)
model_name = "openai/gpt-oss-120b"
response = client.responses.create(
    model=model_name,
    tools=[{"type": "web_search_preview"}],
    input="What updates on the latest SGLang release?"
)

print(response.output_text)
```

## What's Next? ðŸ”®

We are continuously working to push the boundaries of LLM inference. Our future roadmap includes exploring SWA (Sliding Window Attention) optimizations and speculative decoding to further enhance performance.

We invite you to try out the latest version of SGLang and share your feedback. Thank you for being part of our community!